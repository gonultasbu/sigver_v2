{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import scipy.io as sio\n",
    "from scipy.misc import imread\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "from numpy.random import choice\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_curve\n",
    "import sklearn.pipeline as pipeline\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader, random_split\n",
    "from torchvision import transforms as transforms\n",
    "import argparse\n",
    "import pathlib\n",
    "from typing import Dict, Tuple, Any, Optional\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from visdom_logger.logger import VisdomLogger\n",
    "\n",
    "import datasets.util as util\n",
    "from featurelearning.data import TransformDataset\n",
    "import featurelearning.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the architecture\n",
    "class SigNet(nn.Module):\n",
    "    \"\"\" SigNet model, from https://arxiv.org/abs/1705.05787\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(SigNet, self).__init__()\n",
    "\n",
    "        self.feature_space_size = 2048\n",
    "\n",
    "        self.conv_layers = nn.Sequential(OrderedDict([\n",
    "            ('conv1', conv_bn_relu(1, 96, 11, stride=4)),\n",
    "            ('maxpool1', nn.MaxPool2d(3, 2)),\n",
    "            ('conv2', conv_bn_relu(96, 256, 5, pad=2)),\n",
    "            ('maxpool2', nn.MaxPool2d(3, 2)),\n",
    "            ('conv3', conv_bn_relu(256, 384, 3, pad=1)),\n",
    "            ('conv4', conv_bn_relu(384, 384, 3, pad=1)),\n",
    "            ('conv5', conv_bn_relu(384, 256, 3, pad=1)),\n",
    "            ('maxpool3', nn.MaxPool2d(3, 2)),\n",
    "        ]))\n",
    "\n",
    "        self.fc_layers = nn.Sequential(OrderedDict([\n",
    "            ('fc1', linear_bn_relu(256 * 3 * 5, 2048)),\n",
    "            ('fc2', linear_bn_relu(self.feature_space_size, self.feature_space_size)),\n",
    "        ]))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv_layers(inputs)\n",
    "        x = x.view(x.shape[0], 256 * 3 * 5)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def conv_bn_relu(in_channels, out_channels, kernel_size,  stride=1, pad=0):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('conv', nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=False)),\n",
    "        ('bn', nn.BatchNorm2d(out_channels)),\n",
    "        ('relu', nn.ReLU()),\n",
    "    ]))\n",
    "\n",
    "\n",
    "def linear_bn_relu(in_features, out_features):\n",
    "    return nn.Sequential(OrderedDict([\n",
    "        ('fc', nn.Linear(in_features, out_features, bias=False)),  # Bias is added after BN\n",
    "        ('bn', nn.BatchNorm1d(out_features)),\n",
    "        ('relu', nn.ReLU()),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Dataset that applies a transform on the data points on __get__item.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, transform, transform_index=0):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.transform_index = transform_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = self.dataset[item]\n",
    "        img = data[self.transform_index]\n",
    "\n",
    "        return tuple((self.transform(img), *data[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(x, process_function, batch_size, input_size=None):\n",
    "    data = TensorDataset(torch.from_numpy(x))\n",
    "\n",
    "    if input_size is not None:\n",
    "        data_transforms = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.CenterCrop(input_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        data = TransformDataset(data, data_transforms)\n",
    "\n",
    "    data_loader = DataLoader(data, batch_size=batch_size)\n",
    "    result = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            result.append(process_function(batch))\n",
    "    return torch.cat(result).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(base_model: torch.nn.Module,\n",
    "          classification_layer: torch.nn.Module,\n",
    "          forg_layer: torch.nn.Module,\n",
    "          train_loader: torch.utils.data.DataLoader,\n",
    "          val_loader: torch.utils.data.DataLoader,\n",
    "          device: torch.device,\n",
    "          callback: Optional[VisdomLogger],\n",
    "          args: Any,\n",
    "          logdir: Optional[pathlib.Path]):\n",
    "    \"\"\" Trains a network using either SigNet or SigNet-F loss functions on\n",
    "    https://arxiv.org/abs/1705.05787 (e.q. (1) and (4) on the paper)\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_model: torch.nn.Module\n",
    "        The model architecture that \"extract features\" from signatures\n",
    "    classification_layer: torch.nn.Module\n",
    "        The classification layer (from features to predictions of which user\n",
    "        wrote the signature)\n",
    "    forg_layer: torch.nn.Module\n",
    "        The forgery prediction layer (from features to predictions of whether\n",
    "        the signature is a forgery). Only used in args.forg = True\n",
    "    train_loader: torch.utils.data.DataLoader\n",
    "        Iterable that loads the training set (x, y) tuples\n",
    "    val_loader: torch.utils.data.DataLoader\n",
    "        Iterable that loads the validation set (x, y) tuples\n",
    "    device: torch.device\n",
    "        The device (CPU or GPU) to use for training\n",
    "    callback: VisdomLogger (optional)\n",
    "        A callback to report the training progress\n",
    "    args: Namespace\n",
    "        Extra arguments for training: epochs, lr, lr_decay, lr_decay_times, momentum, weight_decay\n",
    "    logdir: str\n",
    "        Where to save the model and training curves\n",
    "    Returns\n",
    "    -------\n",
    "    Dict (str -> tensors)\n",
    "        The trained weights\n",
    "    \"\"\"\n",
    "\n",
    "    # Collect all parameters that need to be optimizer\n",
    "    parameters = list(base_model.parameters()) + list(classification_layer.parameters())\n",
    "    if args.forg:\n",
    "        parameters.extend(forg_layer.parameters())\n",
    "\n",
    "    # Initialize optimizer and learning rate scheduler\n",
    "    optimizer = optim.SGD(parameters, lr=args.lr, momentum=args.momentum,\n",
    "                          nesterov=True, weight_decay=args.weight_decay)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "                                             args.epochs // args.lr_decay_times,\n",
    "                                             args.lr_decay)\n",
    "\n",
    "    best_acc = 0\n",
    "    best_params = get_parameters(base_model, classification_layer, forg_layer)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # Train one epoch; evaluate on validation\n",
    "        train_epoch(train_loader, base_model, classification_layer, forg_layer,\n",
    "                    epoch, optimizer, lr_scheduler, callback, device, args)\n",
    "\n",
    "        val_metrics = test(val_loader, base_model, classification_layer, device, args.forg, forg_layer)\n",
    "        val_acc, val_loss, val_forg_acc, val_forg_loss = val_metrics\n",
    "\n",
    "        # Save the best model only on improvement (early stopping)\n",
    "        if val_acc >= best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_params = get_parameters(base_model, classification_layer, forg_layer)\n",
    "            if logdir is not None:\n",
    "                torch.save(best_params, logdir / 'model_best.pth')\n",
    "\n",
    "        if callback:\n",
    "            callback.scalar('val_loss', epoch + 1, val_loss)\n",
    "            callback.scalar('val_acc', epoch + 1, val_acc)\n",
    "\n",
    "            if args.forg:\n",
    "                callback.scalar('val_forg_loss', epoch + 1, val_forg_loss)\n",
    "                callback.scalar('val_forg_acc', epoch + 1, val_forg_acc)\n",
    "\n",
    "        if args.forg:\n",
    "            print('Epoch {}. Val loss: {:.4f}, Val acc: {:.2f}%,'\n",
    "                  'Val forg loss: {:.4f}, Val forg acc: {:.2f}%'.format(epoch, val_loss,\n",
    "                                                                        val_acc * 100,\n",
    "                                                                        val_forg_loss,\n",
    "                                                                        val_forg_acc * 100))\n",
    "        else:\n",
    "            print('Epoch {}. Val loss: {:.4f}, Val acc: {:.2f}%'.format(epoch, val_loss, val_acc * 100))\n",
    "\n",
    "        if logdir is not None:\n",
    "            current_params = get_parameters(base_model, classification_layer, forg_layer)\n",
    "            torch.save(current_params, logdir / 'model_last.pth')\n",
    "            if callback:\n",
    "                callback.save(logdir / 'train_curves.pickle')\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def copy_to_cpu(weights: Dict[str, Any]):\n",
    "    return OrderedDict([(k, v.cpu()) for k, v in weights.items()])\n",
    "\n",
    "\n",
    "def get_parameters(base_model, classification_layer, forg_layer):\n",
    "    best_params = (copy_to_cpu(base_model.state_dict()),\n",
    "                   copy_to_cpu(classification_layer.state_dict()),\n",
    "                   copy_to_cpu(forg_layer.state_dict()))\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def train_epoch(train_loader: torch.utils.data.DataLoader,\n",
    "                base_model: torch.nn.Module,\n",
    "                classification_layer: torch.nn.Module,\n",
    "                forg_layer: torch.nn.Module,\n",
    "                epoch: int,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                lr_scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "                callback: Optional[VisdomLogger],\n",
    "                device: torch.device,\n",
    "                args: Any):\n",
    "    \"\"\" Trains the network for one epoch\n",
    "        Parameters\n",
    "        ----------\n",
    "        train_loader: torch.utils.data.DataLoader\n",
    "            Iterable that loads the training set (x, y) tuples\n",
    "        base_model: torch.nn.Module\n",
    "            The model architecture that \"extract features\" from signatures\n",
    "        classification_layer: torch.nn.Module\n",
    "            The classification layer (from features to predictions of which user\n",
    "            wrote the signature)\n",
    "        forg_layer: torch.nn.Module\n",
    "            The forgery prediction layer (from features to predictions of whether\n",
    "            the signature is a forgery). Only used in args.forg = True\n",
    "        epoch: int\n",
    "            The current epoch (used for reporting)\n",
    "        optimizer: torch.optim.Optimizer\n",
    "            The optimizer (already initialized)\n",
    "        lr_scheduler: torch.optim.lr_scheduler._LRScheduler\n",
    "            The learning rate scheduler\n",
    "        callback: VisdomLogger (optional)\n",
    "            A callback to report the training progress\n",
    "        device: torch.device\n",
    "            The device (CPU or GPU) to use for training\n",
    "        args: Namespace\n",
    "            Extra arguments used for training:\n",
    "            args.forg: bool\n",
    "                Whether forgeries are being used for training\n",
    "            args.lamb: float\n",
    "                The weight used for the forgery loss (training with forgeries only)\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "\n",
    "    step = 0\n",
    "    n_steps = len(train_loader)\n",
    "    for batch in train_loader:\n",
    "        x, y = batch[0], batch[1]\n",
    "        x = torch.tensor(x, dtype=torch.float).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        yforg = torch.tensor(batch[2], dtype=torch.float).to(device)\n",
    "\n",
    "        # Forward propagation\n",
    "        features = base_model(x)\n",
    "\n",
    "        if args.forg:\n",
    "            if args.loss_type == 'L1':\n",
    "                # Eq (3) in https://arxiv.org/abs/1705.05787\n",
    "                logits = classification_layer(features)\n",
    "                class_loss = F.cross_entropy(logits, y)\n",
    "\n",
    "                forg_logits = forg_layer(features).squeeze()\n",
    "                forg_loss = F.binary_cross_entropy_with_logits(forg_logits, yforg)\n",
    "\n",
    "                loss = (1 - args.lamb) * class_loss\n",
    "                loss += args.lamb * forg_loss\n",
    "            else: \n",
    "                # Eq (4) in https://arxiv.org/abs/1705.05787\n",
    "                logits = classification_layer(features[yforg == 0])\n",
    "                class_loss = F.cross_entropy(logits, y[yforg == 0])\n",
    "\n",
    "                forg_logits = forg_layer(features).squeeze()\n",
    "                forg_loss = F.binary_cross_entropy_with_logits(forg_logits, yforg)\n",
    "\n",
    "                loss = (1 - args.lamb) * class_loss\n",
    "                loss += args.lamb * forg_loss\n",
    "        else:\n",
    "            # Eq (1) in https://arxiv.org/abs/1705.05787\n",
    "            logits = classification_layer(features)\n",
    "            loss = class_loss = F.cross_entropy(logits, y)\n",
    "\n",
    "        # Back propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(optimizer.param_groups[0]['params'], 10)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Logging\n",
    "        if callback and step % 100 == 0:\n",
    "            iteration = epoch + (step / n_steps)\n",
    "            callback.scalar('class_loss', iteration, class_loss.detach())\n",
    "\n",
    "            pred = logits.argmax(1)\n",
    "            if args.loss_type == 'L1': acc = y.eq(pred).float().mean()\n",
    "            else: acc = y[yforg == 0].eq(pred[yforg == 0]).float().mean()\n",
    "            callback.scalar('train_acc', epoch + (step / n_steps), acc.detach())\n",
    "            if args.forg:\n",
    "                forg_pred = forg_logits > 0\n",
    "                forg_acc = yforg.long().eq(forg_pred.long()).float().mean()\n",
    "                callback.scalar('forg_loss', iteration, forg_loss.detach())\n",
    "                callback.scalar('forg_acc', iteration, forg_acc.detach())\n",
    "\n",
    "        step += 1\n",
    "    lr_scheduler.step()\n",
    "\n",
    "\n",
    "def test(val_loader: torch.utils.data.DataLoader,\n",
    "         base_model: torch.nn.Module,\n",
    "         classification_layer: torch.nn.Module,\n",
    "         device: torch.device,\n",
    "         is_forg: bool,\n",
    "         forg_layer: Optional[torch.nn.Module] = None) -> Tuple[float, float, float, float]:\n",
    "    \"\"\" Test the model in a validation/test set\n",
    "    Parameters\n",
    "    ----------\n",
    "    val_loader: torch.utils.data.DataLoader\n",
    "        Iterable that loads the validation set (x, y) tuples\n",
    "    base_model: torch.nn.Module\n",
    "        The model architecture that \"extract features\" from signatures\n",
    "    classification_layer: torch.nn.Module\n",
    "        The classification layer (from features to predictions of which user\n",
    "        wrote the signature)\n",
    "    device: torch.device\n",
    "        The device (CPU or GPU) to use for training\n",
    "    is_forg: bool\n",
    "        Whether or not forgeries are being used for training/testing\n",
    "    forg_layer: torch.nn.Module\n",
    "            The forgery prediction layer (from features to predictions of whether\n",
    "            the signature is a forgery). Only used in is_forg = True\n",
    "    Returns\n",
    "    -------\n",
    "    float, float\n",
    "        The valication accuracy and validation loss\n",
    "    \"\"\"\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "    val_forg_losses = []\n",
    "    val_forg_accs = []\n",
    "    for batch in val_loader:\n",
    "        x, y, yforg = batch[0], batch[1], batch[2]\n",
    "        x = torch.tensor(x, dtype=torch.float).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        yforg = torch.tensor(yforg, dtype=torch.float).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = base_model(x)\n",
    "            logits = classification_layer(features[yforg == 0])\n",
    "\n",
    "            loss = F.cross_entropy(logits, y[yforg == 0])\n",
    "            pred = logits.argmax(1)\n",
    "            acc = y[yforg == 0].eq(pred).float().mean()\n",
    "\n",
    "            if is_forg:\n",
    "                forg_logits = forg_layer(features).squeeze()\n",
    "                forg_loss = F.binary_cross_entropy_with_logits(forg_logits, yforg)\n",
    "                forg_pred = forg_logits > 0\n",
    "                forg_acc = yforg.long().eq(forg_pred.long()).float().mean()\n",
    "\n",
    "                val_forg_losses.append(forg_loss.item())\n",
    "                val_forg_accs.append(forg_acc.item())\n",
    "\n",
    "        val_losses.append(loss.item())\n",
    "        val_accs.append(acc.item())\n",
    "    val_loss = np.mean(val_losses)\n",
    "    val_acc = np.mean(val_accs)\n",
    "    val_forg_loss = np.mean(val_forg_losses) if len(val_forg_losses) > 0 else np.nan\n",
    "    val_forg_acc= np.mean(val_forg_accs) if len(val_forg_accs) > 0 else np.nan\n",
    "\n",
    "    if is_forg: return val_acc.item(), val_loss.item(), val_forg_acc.item(), val_forg_loss.item()\n",
    "    else : return val_acc.item(), val_loss.item(), val_forg_acc, val_forg_loss\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    # Setup logging\n",
    "    logdir = pathlib.Path(args.logdir)\n",
    "    if not logdir.exists():\n",
    "        logdir.mkdir()\n",
    "\n",
    "    if args.visdomport is not None:\n",
    "        logger = VisdomLogger(port=args.visdomport)\n",
    "    else:\n",
    "        logger = None\n",
    "\n",
    "    device = torch.device('cuda', args.gpu_idx) if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print('Using device: {}'.format(device))\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    print('Loading Data')\n",
    "\n",
    "    x, y, yforg, usermapping, filenames = util.load_dataset(args.dataset_path)\n",
    "    data = util.get_subset((x, y, yforg), subset=range(*args.users))\n",
    "    if not args.forg:\n",
    "        data = util.remove_forgeries(data, forg_idx=2)\n",
    "\n",
    "    train_loader, val_loader = setup_data_loaders(data, args.batch_size, args.input_size)\n",
    "\n",
    "    print('Initializing Model')\n",
    "\n",
    "    n_classes = len(np.unique(data[1]))\n",
    "\n",
    "    base_model = models.available_models[args.model]().to(device)\n",
    "    classification_layer = nn.Linear(base_model.feature_space_size, n_classes).to(device)\n",
    "    if args.forg:\n",
    "        forg_layer = nn.Linear(base_model.feature_space_size, 1).to(device)\n",
    "    else:\n",
    "        forg_layer = nn.Module()  # Stub module with no parameters\n",
    "\n",
    "    if args.test:\n",
    "        print('Testing')\n",
    "        base_model_params, classification_params, forg_params = torch.load(args.checkpoint)\n",
    "        base_model.load_state_dict(base_model_params)\n",
    "\n",
    "        classification_layer.load_state_dict(classification_params)\n",
    "        if args.forg:\n",
    "            forg_layer.load_state_dict(forg_params)\n",
    "        val_acc, val_loss, val_forg_acc, val_forg_loss = test(val_loader, base_model, classification_layer,\n",
    "                                                              device, args.forg, forg_layer)\n",
    "        if args.forg:\n",
    "            print('Val loss: {:.4f}, Val acc: {:.2f}%,'\n",
    "                  'Val forg loss: {:.4f}, Val forg acc: {:.2f}%'.format(val_loss,\n",
    "                                                                        val_acc * 100,\n",
    "                                                                        val_forg_loss,\n",
    "                                                                        val_forg_acc * 100))\n",
    "        else:\n",
    "            print('Val loss: {:.4f}, Val acc: {:.2f}%'.format(val_loss, val_acc * 100))\n",
    "\n",
    "    else:\n",
    "        print('Training')\n",
    "        train(base_model, classification_layer, forg_layer, train_loader, val_loader,\n",
    "              device, logger, args, logdir)\n",
    "\n",
    "\n",
    "def setup_data_loaders(data, batch_size, input_size):\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(data[1])\n",
    "    data = TensorDataset(torch.from_numpy(data[0]), torch.from_numpy(y), torch.from_numpy(data[2]))\n",
    "    train_size = int(0.9 * len(data))\n",
    "    sizes = (train_size, len(data) - train_size)\n",
    "    train_set, test_set = random_split(data, sizes)\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    train_set = TransformDataset(train_set, train_transforms)\n",
    "    val_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    test_set = TransformDataset(test_set, val_transforms)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argparser = argparse.ArgumentParser('Train Signet/F')\n",
    "argparser.add_argument('--dataset-path', help='Path containing a numpy file with images and labels')\n",
    "argparser.add_argument('--input-size', help='Input size (cropped)', nargs=2, type=int, default=(150, 220))\n",
    "argparser.add_argument('--users', nargs=2, type=int, default=(350, 881))\n",
    "\n",
    "argparser.add_argument('--model', help='Model architecture', choices=models.available_models, required=True)\n",
    "argparser.add_argument('--batch-size', help='Batch size', type=int, default=32)\n",
    "argparser.add_argument('--lr', help='learning rate', default=0.001, type=float)\n",
    "argparser.add_argument('--lr-decay', help='learning rate decay (multiplier)', default=0.1, type=float)\n",
    "argparser.add_argument('--lr-decay-times', help='number of times learning rate decays', default=3, type=float)\n",
    "argparser.add_argument('--momentum', help='momentum', default=0.90, type=float)\n",
    "argparser.add_argument('--weight-decay', help='Weight Decay', default=1e-4, type=float)\n",
    "argparser.add_argument('--epochs', help='Number of epochs', default=20, type=int)\n",
    "argparser.add_argument('--checkpoint', help='starting weights (pth file)')\n",
    "argparser.add_argument('--test', action='store_true')\n",
    "\n",
    "argparser.add_argument('--seed', default=42, type=int)\n",
    "\n",
    "argparser.add_argument('--forg', dest='forg', action='store_true')\n",
    "argparser.add_argument('--lamb', type=float, help='Lambda for trading of user classification '\n",
    "                                                  'and forgery classification')\n",
    "argparser.add_argument('--loss-type', help='L1 or L2 loss, implemented on paper Eq(3) or Eq(4)', default='L2', type=str)\n",
    "\n",
    "argparser.add_argument('--gpu-idx', default=0, type=int)\n",
    "argparser.add_argument('--logdir', help='logdir', required=True)\n",
    "argparser.add_argument('--visdomport', help='Visdom port (plotting)', type=int)\n",
    "\n",
    "argparser.set_defaults(forg=False, test=False)\n",
    "arguments = argparser.parse_args()\n",
    "print(arguments)\n",
    "\n",
    "main(arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
